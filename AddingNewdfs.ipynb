{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48918123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# this is the path to the folder where you have the CSVs, NO OTHER CSVs SHOULD BE PRESENT\n",
    "# please make sure this path is not inside the scope of GitHub so we do not go over on data for our repo\n",
    "path = r'C:\\RotorCraftData\\CSV'\n",
    "pattern = r'.*2023\\.06\\.15.*\\.csv$'\n",
    "\n",
    "# this imports a list of columns that was saved after the removal of variance on a single CSV, this list will be used to define which columns to read in\n",
    "with open('./src/use_cols.pkl', 'rb') as f:\n",
    "    use_cols = pickle.load(f)\n",
    "\n",
    "# the data will be labeled using the information from the flight logs\n",
    "label_table = pd.DataFrame({\n",
    "    'Date': ['2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15'],  # Replace with actual dates of maneuvers\n",
    "    # Replace with actual start time of maneuvers\n",
    "    'StartTime': ['13:22:15.0', '13:25:25.0', '13:29:25.0', '11:56:25.0', '11:58:03.0', '11:59:51.0', '16:10:04.0', '16:11:41.0', '16:14:20.0', '13:43:12.0', '13:44:10.0', '13:45:19.0', '12:08:11.0', '12:09:31.0', '12:10:51.0', '16:34:28.0', '16:35:06.0', '16:38:26.0'],\n",
    "    # Replace with actual end time of maneuvers\n",
    "    'EndTime': ['13:22:25.0', '13:25:38.0', '13:29:40.0', '11:56:38.0', '11:58:24.0', '12:00:00.0', '16:10:12.0', '16:11:46.0', '16:14:29.0', '13:43:35.0', '13:44:18.0', '13:45:30.0', '12:08:35.0', '12:09:52.0', '12:11:18.0', '16:34:42.0', '16:35:27.0', '16:38:36.0'],\n",
    "    'Label': ['Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G']  # Replace with maneuver names\n",
    "})\n",
    "\n",
    "# convert date, start time, and end time columns to datetime type\n",
    "label_table['Date'] = pd.to_datetime(label_table['Date'])\n",
    "label_table['StartTime'] = pd.to_datetime(\n",
    "    label_table['StartTime'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "label_table['EndTime'] = pd.to_datetime(\n",
    "    label_table['EndTime'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "\n",
    "\n",
    "def combine_csv_files(csv_directory, columns_to_use, label_df):\n",
    "    # get list of CSV file paths in the directory\n",
    "    csv_files = [os.path.join(csv_directory, filename) for filename in os.listdir(\n",
    "        csv_directory) if re.match(pattern, filename)]\n",
    "    # create an empty dataframe to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # iterate over each CSV file\n",
    "    for file in csv_files:\n",
    "        # read CSV file and select desired columns\n",
    "        temp_df = pd.read_csv(file, usecols=columns_to_use, names=columns_to_use, skiprows=3, skipfooter=1, engine='python')\n",
    "        # drop rows that Elapsed Time are mostly null, these are the breaks in simulation\n",
    "        temp_df.dropna(subset=['Elapsed Time'], inplace=True)\n",
    "        # temp_df.drop(['Elapsed Time'], inplace=True)\n",
    "        temp_df.dropna(inplace=True)\n",
    "        # concatenate the temporary dataframe with the running dataframe\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # convert the time column on original df to correct format\n",
    "    combined_df['System UTC Time'] = pd.to_datetime(\n",
    "    combined_df['System UTC Time'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "    # convert the date column on original df to correct format\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "    \n",
    "    # apply the labeling to the dataset\n",
    "    for _, row in label_df.iterrows():\n",
    "        # extract date, start time, and end time from the current row\n",
    "        date = row['Date']\n",
    "        start_time = row['StartTime']\n",
    "        end_time = row['EndTime']\n",
    "        label = row['Label']\n",
    "\n",
    "        # filter the existing dataset based on matching date and within start time and end time\n",
    "        filter_condition = (combined_df['Date'] == date) & (\n",
    "            combined_df['System UTC Time'].between(start_time, end_time))\n",
    "        combined_df.loc[filter_condition, 'Label'] = label\n",
    "    dummies_df = pd.get_dummies(combined_df['Label'], dummy_na=False)\n",
    "    dummies_df = dummies_df.astype(int)\n",
    "    combined_df = pd.concat([combined_df, dummies_df], axis=1)\n",
    "    # Convert the time column to pandas datetime format if it's not already in that format\n",
    "    combined_df['System UTC Time'] = pd.to_datetime(combined_df['System UTC Time'], format='%H:%M:%S.%f')\n",
    "\n",
    "    # Set the start and end time range\n",
    "    start_time = pd.to_datetime('11:56:25.0', format='%H:%M:%S.%f')\n",
    "    end_time = pd.to_datetime('16:38:26.0', format='%H:%M:%S.%f')\n",
    "\n",
    "    # Filter the DataFrame to include rows between the start and end times\n",
    "    combined_df = combined_df[(combined_df['System UTC Time'] >= start_time) & (combined_df['System UTC Time'] <= end_time)].copy()\n",
    "\n",
    "    combined_df.drop(['Elapsed Time', 'Date', 'System UTC Time'], inplace=True, axis=1)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# this calls the function from above that cleans and creates dummy variables for our target variables\n",
    "df = combine_csv_files(path, use_cols, label_table)\n",
    "# this will create a pickle file with the working dataframe in your directory with the original CSV files\n",
    "# you will not need to run this script again, as we will load in the dataframe from the pickle file\n",
    "with open(f'{path}/working_df2.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1566381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258905, 32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c700c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# this is the path to the folder where you have the CSVs, NO OTHER CSVs SHOULD BE PRESENT\n",
    "# please make sure this path is not inside the scope of GitHub so we do not go over on data for our repo\n",
    "path = r'C:\\RotorCraftData\\CSV'\n",
    "pattern = r'.*2023\\.06\\.15.*\\.csv$'\n",
    "\n",
    "# this imports a list of columns that was saved after the removal of variance on a single CSV, this list will be used to define which columns to read in\n",
    "with open('./src/use_cols_aws.pkl', 'rb') as f:\n",
    "    use_cols = pickle.load(f)\n",
    "\n",
    "# the data will be labeled using the information from the flight logs\n",
    "label_table = pd.DataFrame({\n",
    "    'Date': ['2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15'], \n",
    "    # Replace with actual start time of maneuvers\n",
    "    'StartTime': ['13:22:15.0', '13:25:25.0', '13:29:25.0', '11:56:25.0', '11:58:03.0', '11:59:51.0', '16:10:04.0', '16:11:41.0', '16:14:20.0', '13:43:12.0', '13:44:10.0', '13:45:19.0', '12:08:11.0', '12:09:31.0', '12:10:51.0', '16:34:28.0', '16:35:06.0', '16:38:26.0'],\n",
    "    # Replace with actual end time of maneuvers\n",
    "    'EndTime': ['13:22:25.0', '13:25:38.0', '13:29:40.0', '11:56:38.0', '11:58:24.0', '12:00:00.0', '16:10:12.0', '16:11:46.0', '16:14:29.0', '13:43:35.0', '13:44:18.0', '13:45:30.0', '12:08:35.0', '12:09:52.0', '12:11:18.0', '16:34:42.0', '16:35:27.0', '16:38:36.0'],\n",
    "    'Label': ['Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G']  \n",
    "})\n",
    "\n",
    "# convert date, start time, and end time columns to datetime type\n",
    "label_table['Date'] = pd.to_datetime(label_table['Date'])\n",
    "label_table['StartTime'] = pd.to_datetime(\n",
    "    label_table['StartTime'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "label_table['EndTime'] = pd.to_datetime(\n",
    "    label_table['EndTime'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "\n",
    "\n",
    "def combine_csv_files(csv_directory, columns_to_use, label_df):\n",
    "    # get list of CSV file paths in the directory\n",
    "    csv_files = [os.path.join(csv_directory, filename) for filename in os.listdir(\n",
    "        csv_directory) if re.match(pattern, filename)]\n",
    "    # create an empty dataframe to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # iterate over each CSV file\n",
    "    for file in csv_files:\n",
    "        # read CSV file and select desired columns\n",
    "        temp_df = pd.read_csv(file, usecols=columns_to_use, names=columns_to_use, skiprows=3, skipfooter=1, engine='python')\n",
    "        # drop rows that Elapsed Time are mostly null, these are the breaks in simulation\n",
    "        temp_df.dropna(subset=['Elapsed Time'], inplace=True)\n",
    "        # temp_df.drop(['Elapsed Time'], inplace=True)\n",
    "        temp_df.dropna(inplace=True)\n",
    "        # concatenate the temporary dataframe with the running dataframe\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # convert the time column on original df to correct format\n",
    "    combined_df['System UTC Time'] = pd.to_datetime(\n",
    "    combined_df['System UTC Time'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "    # convert the date column on original df to correct format\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "    \n",
    "    # apply the labeling to the dataset\n",
    "    for _, row in label_df.iterrows():\n",
    "        # extract date, start time, and end time from the current row\n",
    "        date = row['Date']\n",
    "        start_time = row['StartTime']\n",
    "        end_time = row['EndTime']\n",
    "        label = row['Label']\n",
    "\n",
    "        # filter the existing dataset based on matching date and within start time and end time\n",
    "        filter_condition = (combined_df['Date'] == date) & (\n",
    "            combined_df['System UTC Time'].between(start_time, end_time))\n",
    "        combined_df.loc[filter_condition, 'Label'] = label\n",
    "    dummies_df = pd.get_dummies(combined_df['Label'], dummy_na=False)\n",
    "    dummies_df = dummies_df.astype(int)\n",
    "    combined_df = pd.concat([combined_df, dummies_df], axis=1)\n",
    "    # Convert the time column to pandas datetime format if it's not already in that format\n",
    "    combined_df['System UTC Time'] = pd.to_datetime(combined_df['System UTC Time'], format='%H:%M:%S.%f')\n",
    "\n",
    "    # Set the start and end time range\n",
    "    start_time = pd.to_datetime('11:56:25.0', format='%H:%M:%S.%f')\n",
    "    end_time = pd.to_datetime('16:38:26.0', format='%H:%M:%S.%f')\n",
    "\n",
    "    # Filter the DataFrame to include rows between the start and end times\n",
    "    combined_df = combined_df[(combined_df['System UTC Time'] >= start_time) & (combined_df['System UTC Time'] <= end_time)].copy()\n",
    "\n",
    "    combined_df.drop(['Elapsed Time', 'Date', 'System UTC Time', 'Label'], inplace=True, axis=1)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# this calls the function from above that cleans and creates dummy variables for our target variables\n",
    "df = combine_csv_files(path, use_cols, label_table)\n",
    "# this will create a pickle file with the working dataframe in your directory with the original CSV files\n",
    "# you will not need to run this script again, as we will load in the dataframe from the pickle file\n",
    "with open(f'{path}/working_df_aws.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf173ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258905, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e2c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb408c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
