{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fcb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3fe0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the path to the folder where you have the CSVs, NO OTHER CSVs SHOULD BE PRESENT\n",
    "# please make sure this path is not inside the scope of GitHub so we do not go over on data for our repo\n",
    "path = r'C:\\RotorCraftData\\CSV'\n",
    "pattern = r'.*2023\\.06\\.15.*\\.csv$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d2a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this imports a list of columns that was saved after the removal of variance on a single CSV, this list will be used to define which columns to read in\n",
    "with open('./src/use_cols.pkl', 'rb') as f:\n",
    "    use_cols = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21952b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data will be labeled using the information from the flight logs\n",
    "label_table = pd.DataFrame({\n",
    "    'Date': ['2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15', '2023-06-15'],  # Replace with actual dates of maneuvers\n",
    "    # Replace with actual start time of maneuvers\n",
    "    'StartTime': ['13:22:15.0', '13:25:25.0', '13:29:25.0', '11:56:25.0', '11:58:03.0', '11:59:51.0', '16:10:04.0', '16:11:41.0', '16:14:20.0', '13:43:12.0', '13:44:10.0', '13:45:19.0', '12:08:11.0', '12:09:31.0', '12:10:51.0', '16:34:28.0', '16:35:06.0', '16:38:26.0'],\n",
    "    # Replace with actual end time of maneuvers\n",
    "    'EndTime': ['13:22:25.0', '13:25:38.0', '13:29:40.0', '11:56:38.0', '11:58:24.0', '12:00:00.0', '16:10:12.0', '16:11:46.0', '16:14:29.0', '13:43:35.0', '13:44:18.0', '13:45:30.0', '12:08:35.0', '12:09:52.0', '12:11:18.0', '16:34:42.0', '16:35:27.0', '16:38:36.0'],\n",
    "    'Label': ['Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'Dynamic Rollover', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G', 'LOW-G']  # Replace with maneuver names\n",
    "})\n",
    "\n",
    "# convert date, start time, and end time columns to datetime type\n",
    "label_table['Date'] = pd.to_datetime(label_table['Date'])\n",
    "label_table['StartTime'] = pd.to_datetime(\n",
    "    label_table['StartTime'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "label_table['EndTime'] = pd.to_datetime(\n",
    "    label_table['EndTime'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "\n",
    "\n",
    "def combine_csv_files(csv_directory, columns_to_use, label_df):\n",
    "    # get list of CSV file paths in the directory\n",
    "    csv_files = [os.path.join(csv_directory, filename) for filename in os.listdir(\n",
    "        csv_directory) if re.match(pattern, filename)]\n",
    "    # create an empty dataframe to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # iterate over each CSV file\n",
    "    for file in csv_files:\n",
    "        # read CSV file and select desired columns\n",
    "        temp_df = pd.read_csv(file, usecols=columns_to_use, names=columns_to_use, skiprows=3, skipfooter=1, engine='python')\n",
    "        # drop rows that Elapsed Time are mostly null, these are the breaks in simulation\n",
    "        temp_df.dropna(subset=['Elapsed Time'], inplace=True)\n",
    "        # temp_df.drop(['Elapsed Time'], inplace=True)\n",
    "        temp_df.dropna(inplace=True)\n",
    "        # concatenate the temporary dataframe with the running dataframe\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # convert the time column on original df to correct format\n",
    "    combined_df['System UTC Time'] = pd.to_datetime(\n",
    "    combined_df['System UTC Time'], format='%H:%M:%S.%f').dt.strftime('%H:%M:%S.%f')\n",
    "    # convert the date column on original df to correct format\n",
    "    combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
    "    \n",
    "    # apply the labeling to the dataset\n",
    "    for _, row in label_df.iterrows():\n",
    "        # extract date, start time, and end time from the current row\n",
    "        date = row['Date']\n",
    "        start_time = row['StartTime']\n",
    "        end_time = row['EndTime']\n",
    "        label = row['Label']\n",
    "\n",
    "        # filter the existing dataset based on matching date and within start time and end time\n",
    "        filter_condition = (combined_df['Date'] == date) & (\n",
    "            combined_df['System UTC Time'].between(start_time, end_time))\n",
    "        combined_df.loc[filter_condition, 'Label'] = label\n",
    "    dummies_df = pd.get_dummies(combined_df['Label'], dummy_na=False)\n",
    "    dummies_df = dummies_df.astype(int)\n",
    "    combined_df = pd.concat([combined_df, dummies_df], axis=1)\n",
    "    combined_df.drop(['Elapsed Time', 'Date', 'System UTC Time'], inplace=True, axis=1)\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf1e326",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Altitude(MSL)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Altitude(MSL)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m combine_csv_files(path, use_cols, label_table)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# dataframe is created with one column having the incorrect type\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAltitude(MSL)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAltitude(MSL)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# this will create a pickle file with the working dataframe in your directory with the original CSV files\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# you will not need to run this script again, as we will load in the dataframe from the pickle file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/working_df.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Altitude(MSL)'"
     ]
    }
   ],
   "source": [
    "\n",
    "# this calls the function from above that cleans and creates dummy variables for our target variables\n",
    "df = combine_csv_files(path, use_cols, label_table)\n",
    "# dataframe is created with one column having the incorrect type\n",
    "df['Altitude(MSL)'] = df['Altitude(MSL)'].astype('float')\n",
    "# this will create a pickle file with the working dataframe in your directory with the original CSV files\n",
    "# you will not need to run this script again, as we will load in the dataframe from the pickle file\n",
    "with open(f'{path}/working_df.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7994c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487470, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ff2d8",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c162d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#https://www.edureka.co/blog/principal-component-analysis/\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Step 1 - import required packages\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcParams\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#https://www.edureka.co/blog/principal-component-analysis/\n",
    "#Step 1 - import required packages\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import rcParams\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2- Select df. I will be using RCdf(136 var). That dataset has\n",
    "# only numeric values and the date which I could convert to float. System UTC time could not be\n",
    "#converted to float. I need to research more. \n",
    "RCdf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3 - formatting the data. It has been formatted already. Excluding response variables.  , 'label_0','label_Dynamic Rollover','label_LOW-G'.\n",
    "# df2 has only float a#nd int data columns. Dropping object and uint8.\n",
    "df2 = RCdf.drop(['label_Dynamic Rollover','label_LOW-G'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dfc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Standardization. It run using df2\n",
    "X_std = StandardScaler().fit_transform(df2)\n",
    "#X_std\n",
    "pd.options.display.max_columns = None\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e511e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Compute covariance matrix.\n",
    "#These values show the distribution magnitude and direction of multivariate data in a multidimensional space and \n",
    "#can allow you to gather information about how data spreads among two dimensions.\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_matrix = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "#print('Covariance matrix n%s' %cov_mat)\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Calculate eigenvectors and eigenvalues\n",
    "#Calculating eigenvectors and eigenvalues on covariance matrix.\n",
    "#eigenvalue function returns two type of arrays, one dimensional array representing the eigenvalues in \n",
    "#the position of the input and another two dimensional array giving the eigenvector corresponding to the columns in the input matrix.\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "print('Eigenvectors n%s' %eig_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nEigenvalues n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c31ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Compute the feature vector.  rearrange the eigenvalues in descending order. \n",
    "#This represents the significance of the principal components in descending order:\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])\n",
    "#The top 10 eigenvalues adds up to 71% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59632d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Use the PCA() function to reduce the dimensionality of the data set\n",
    "pca = PCA()\n",
    "pca.fit_transform(df2)\n",
    "print(pca.explained_variance_ratio_)\n",
    "len(pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9:Projecting the variance to the Principle Components\n",
    "pca = PCA().fit(X_std)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Scree plot of commulative expalined variance to the PCs')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
