{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffa9175",
   "metadata": {},
   "source": [
    "# Logistic Regression Model Analysis Walkthrough (Low Gravity)\n",
    "\n",
    "Logistic regression is a statistical method for predicting binary outcomes from data. It predicts the probability of an instance belonging to the default class, which can be transformed into a binary outcome via a threshold (e.g., if the output probability is > 0.5, classify as class 1, otherwise class 0). The core principle of logistic regression is to establish a relationship between features and the probability of particular outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db48341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore specific warning categories\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc2ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the path to your pickle file (should be the same location as CSVs)\n",
    "path = Path('../data')\n",
    "\n",
    "with open(path / 'low_g_pandas_2.0.2.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fe102",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "We begin by splitting our dataset into a training and testing set. This process ensures that we have a distinct set of data to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5bae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define independent variables and dependent variable\n",
    "maneuver = 'LOW-G'\n",
    "X = df.drop(maneuver, axis=1)\n",
    "y = df[maneuver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d57a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29bdbd0",
   "metadata": {},
   "source": [
    "## Build and Fit the Model\n",
    "We will set up a parameter grid with the best parameters. These parameters were developed in the modeling directory in the decision tree (dynamic rollover) file. For more information on the training and tuning of this model please refer to the modeling file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21223c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented pipelines and parameters represent alternatives that were considered but not chosen for the final model.\n",
    "# They provide context on what configurations were explored during the model selection process.\n",
    "\n",
    "#  pipe_with_rus = ImbPipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('under', RandomUnderSampler(random_state=42)),\n",
    "#     ('lr', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# pipe_without_rus = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('lr', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# params = [\n",
    "#     {\n",
    "#         'pipeline': [pipe_with_rus, pipe_without_rus],  # Both pipelines are tested\n",
    "#         'pipeline__lr__penalty': ['l2', 'none'],\n",
    "#         'pipeline__lr__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#         'pipeline__lr__class_weight': ['balanced'],\n",
    "#         'pipeline__lr__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "#         'pipeline__lr__fit_intercept': [True, False],\n",
    "#         'pipeline__lr__random_state': [42],\n",
    "#         'pipeline__lr__n_jobs': [-1],\n",
    "#         'pipeline__lr__max_iter': [10000000]\n",
    "#     },\n",
    "#     {\n",
    "#         'pipeline': [pipe_with_rus, pipe_without_rus],  # Both pipelines are tested\n",
    "#         'pipeline__lr__penalty': ['l1'],\n",
    "#         'pipeline__lr__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#         'pipeline__lr__class_weight': ['balanced'],\n",
    "#         'pipeline__lr__solver': ['liblinear', 'saga'],\n",
    "#         'pipeline__lr__fit_intercept': [True, False],\n",
    "#         'pipeline__lr__random_state': [42],\n",
    "#         'pipeline__lr__n_jobs': [-1],\n",
    "#         'pipeline__lr__max_iter': [10000000]\n",
    "#     },\n",
    "#     {\n",
    "#         'pipeline': [pipe_with_rus, pipe_without_rus],  # Both pipelines are tested\n",
    "#         'pipeline__lr__penalty': ['elasticnet'],\n",
    "#         'pipeline__lr__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "#         'pipeline__lr__class_weight': ['balanced'],\n",
    "#         'pipeline__lr__solver': ['saga'],\n",
    "#         'pipeline__lr__l1_ratio': [0, 0.25, 0.5, 0.75, 1],\n",
    "#         'pipeline__lr__fit_intercept': [True, False],\n",
    "#         'pipeline__lr__random_state': [42],\n",
    "#         'pipeline__lr__n_jobs': [-1],\n",
    "#         'pipeline__lr__max_iter': [10000000]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# parameters for the optimal model\n",
    "params = {\n",
    "    'lr__penalty': ['l1'],\n",
    "    'lr__C': [1],\n",
    "    'lr__class_weight': ['balanced'],\n",
    "    'lr__solver': ['liblinear'],\n",
    "    'lr__fit_intercept': [True],\n",
    "    'lr__random_state': [42],\n",
    "    'lr__n_jobs': [-1],\n",
    "    'lr__max_iter': [10000000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c6b79",
   "metadata": {},
   "source": [
    "## Make Predictions for Model Evaluation\n",
    "\n",
    "Note: this model takes a very long time to train likely due to the non-linear nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b760b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search with cross-validation\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=params, cv=strat_k_fold, scoring=f1_scorer)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ab9d5",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "These visualizations will provide a comprehensive understanding of the model's performance, the features that drive decisions, and how the model's performance evolves as more data is added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60c3bf",
   "metadata": {},
   "source": [
    "### Coefficient Analysis\n",
    "The coefficients of a logistic regression model represent the log-odds of the outcome variable. In simpler terms, a coefficient indicates how the log-odds of the dependent variable change given a one-unit change in the predictor variable, holding all other predictors constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35745a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = grid_search.best_estimator_.named_steps['lr'].coef_[0]\n",
    "features = X_train.columns\n",
    "coef_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=\"Coefficient\", y=\"Feature\", data=coef_df)\n",
    "plt.title(\"Coefficient Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90917985",
   "metadata": {},
   "source": [
    "### Feature Importances\n",
    "\n",
    "Feature importances provide insights into which variables or features have the most influence in making a decision in a machine learning model. In the context of decision trees and ensemble methods based on trees, like random forests, feature importance is computed by looking at the splits in the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve feature importances\n",
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.barh(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.yticks(range(X_train.shape[1]), [features[i] for i in indices])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fbe21",
   "metadata": {},
   "source": [
    "- Positive Coefficients: Indicate that as the predictor variable increases, the probability of the outcome occurring also increases.\n",
    "- Negative Coefficients: Indicate that as the predictor variable increases, the probability of the outcome occurring decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98b60f",
   "metadata": {},
   "source": [
    "### Probability Distribution\n",
    "\n",
    "In logistic regression, the outcome is a probability that the given input point belongs to a particular class, which is transformed into a binary outcome via a threshold (e.g., if the output probability is greater than 0.5, classify as class 1, otherwise class 0). The probability distribution plot visualizes the distribution of probabilities predicted by the model for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a68949",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.distplot(y_pred_prob[y_test == 1], color='b', label='Class 1')\n",
    "sns.distplot(y_pred_prob[y_test == 0], color='r', label='Class 0')\n",
    "plt.legend()\n",
    "plt.title('Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c93cd",
   "metadata": {},
   "source": [
    "Overlap between the distributions of the two classes suggests areas where the model might be uncertain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e664a",
   "metadata": {},
   "source": [
    "### Residuals Analysis\n",
    "\n",
    "Residuals in the context of logistic regression are the differences between the observed binary outcomes and the probabilities predicted by the model. Residual analysis is used to assess the goodness-of-fit of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21628a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_prob\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred_prob, y=residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs. Predicted Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0fb2d",
   "metadata": {},
   "source": [
    "Residuals vs. Predicted Probability Plot: This plot is a scatterplot of residuals on the y-axis and predicted probabilities on the x-axis. The plot can reveal patterns indicating that the model is not fitting certain parts of the data well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
